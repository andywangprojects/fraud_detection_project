<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Credit Card Transactions Fraud Detection Report</title>
</head>
<body>
  <h1>📊 Credit Card Transaction Fraud Detection Data Analysis</h1>
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Fraud Detection EDA Dashboard</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
      color: #333;
      background-color: #f9f9f9;
    }
    h1 {
      text-align: center;
      color: #1a73e8;
    }
    h2 {
      margin-top: 40px;
      color: #2c3e50;
      border-bottom: 2px solid #ccc;
      padding-bottom: 5px;
    }
    h3 {
      margin-top: 25px;
      color: #444;
    }
    a {
      color: #1a73e8;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      display: block;
      margin: 10px auto 30px;
      border: 1px solid #ccc;
      box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
    }
    p {
      max-width: 800px;
      margin: 0 auto 15px;
      text-align: justify;
    }
    hr {
      margin: 40px 0;
      border: none;
      border-top: 1px solid #ddd;
    }
    .footer {
      text-align: center;
      margin-top: 60px;
      font-style: italic;
      color: #777;
    }
  </style>
</head>
<body>

<h2>📦 Data Resource</h2>
<p class="left-aligned">
  Original dataset from
  <a href="https://www.kaggle.com/kartik2112/fraud-detection" target="_blank">
    Kaggle: Credit Card Fraud Detection
  </a>
</p>

<h2>📄 Interactive EDA Report</h2>
<p class="left-aligned">
  Access the full exploratory data analysis report here:<br>
  <a href="train_eda_report.html" target="_blank">📄 View Full EDA Report for Train Data (HTML)</a>
  <a href="test_eda_report.html" target="_blank">📄 View Full EDA Report for Test Data (HTML)</a>
</p>

 <h2>📊 Class Imbalance Analysis </h2>

<h3>🧾 Distribution of the Data</h3>
<p>
The  dataset contains a total of <strong>1,844,974</strong> credit card transaction records labeled as <code>is_fraud</code>, indicating whether a transaction is fraudulent (1) or not (0).
</p>
<ul>
  <li><strong>Non-Fraudulent transactions (label 0):</strong> 1,835,624 (<strong>99.4941%</strong>)</li>
  <li><strong>Fraudulent transactions (label 1):</strong> 9,350 (<strong>0.5059%</strong>)</li>
</ul>
<p>
This extreme class imbalance poses a significant challenge for classification models. While the dataset is dominated by legitimate transactions, the minority class (fraudulent cases) is of primary importance, especially in practical fraud detection systems. 
</p>
  
<h2>⚠️ Why Tackling Imbalanced Data is Important</h3>
<p>
For a classification model, especially in domains such as fraud detection, medical diagnostics, and spam filtering, rare but high-impact cases are often the primary concern. However, if imbalanced data is not properly addressed during training, the model may appear to perform well—achieving high overall accuracy—while completely failing to detect the minority class. This results in misleading evaluation metrics and significantly reduces the model’s practical value.In such datasets, the minority class often behaves like an outlier due to its sparse distribution in the feature space. Many models interpret these rare patterns as noise or unrepresentative signals, leading them to underfit or ignore the minority class. Since most algorithms aim to minimize overall error, they are naturally biased toward the majority class.Consequently, imbalanced data can cause the model to overfit the dominant class while neglecting the minority class. Addressing this imbalance is therefore critical to building models that are not only statistically sound but also effective in real-world decision-making.
</p>

<h3>📏 Metrics & Methods for Imbalanced Data</h3>
<ul>
  <li><strong>SMOTE (Synthetic Minority Over-sampling Technique):</strong> A technique that creates synthetic samples of the minority class by interpolating between existing examples, helping the model learn core patterns without overfitting to duplicates.</li>
  <li><strong>Undersampling:</strong> Randomly removes samples from the majority class to balance the dataset. 
  <li><strong>F<sub>2</sub> Score:</strong> A weighted version of the F1-score that prioritizes <strong>recall</strong> over precision, making it more suitable for fraud detection where missing a fraud is more costly than false alerts.</li>
</ul>
  
<h3>📌 Why SMOTE + Undersampling + <code>class_weight='balanced'</code> should be avoided in Logistic Regression but works in Random Forest</h2>

When handling imbalanced datasets, SMOTE (oversampling), undersampling, and <code>class_weight='balanced'</code> are commonly used. However, using all three together in <strong>Logistic Regression</strong> may harm model performance, while doing so in <strong>Random Forest</strong> is acceptable.</p>

<h4>⚠️ In Logistic Regression:</h3>
<ul>
   <li>Logistic Regression tries to find one straight boundary (line or plane) to separate classes.</li>
  <li>SMOTE + undersampling already balance the dataset.</li>
  <li>Adding <code>class_weight='balanced'</code> again makes the model <strong>overcompensate</strong> for the minority class.</li>
  <li>This shifts the decision boundary too far toward the majority class, resulting in <strong>too many false fraud predictions</strong>.</li>
  <li>The model may look good on resampled data, but performs poorly on real-world data (which is still highly imbalanced).</li>
</ul>

<h4>✅ In Random Forest:</h3>
<ul>
  <li>Random Forest builds many small trees on random subsets of data (bootstrapped samples).</li>
  <li>It doesn't rely on one fixed boundary — it uses <strong>majority voting</strong> to make predictions.</li>
  <li>SMOTE adds some variety, undersampling reduces bias, and class weights guide tree splits — but Random Forest can <strong>handle all of them together</strong>.</li>
  <li>Its ensemble nature helps absorb noise and balance different perspectives.</li>
</ul>
<hr>

<h3>🎯 Why SMOTE=0.01 and Undersampling (0.2 / 0.15) are good choices for my model building in consideratioin of the  dataset I used ?</h2>

In Train data, there are 7,506 fraud samples out of 1,296,675 transactions (≈ 1:172).
<ul>

<h4>✅ Why SMOTE=0.01 is effective:</h3>
<ol>
  <li><strong>Minimizes synthetic noise:</strong> A small amount of oversampling avoids flooding the model with artificial fraud cases, which could blur the boundary.<br>
  </li>
  <li><strong>Improves class boundary coverage:</strong> Even light oversampling helps highlight rare fraud patterns near the decision boundary.<br>
  </li>
</ol>

<h4>✅ Why moderate undersampling helps:</h3>
<ol>
  <li><strong>Retains majority class diversity:</strong> Reducing the majority class to 15–20% helps balance the data while keeping enough real transaction information.<br>
  </li>
  <li><strong>Improves recall and reduces overfitting:</strong> When combined with minimal SMOTE, it allows models to generalize better and boost PR-AUC.<br>
  </li>
</ol>

<h3>🔍 Model-specific notes:</h3>
<ul>
  <li><strong>Random Forest:</strong> Tolerates minor synthetic noise well; SMOTE=0.01 and undersampling=0.2 is a balanced setup for robustness.</li>
  <li><strong>Logistic Regression:</strong> More sensitive to class distribution and probability calibration. SMOTE=0.01 with undersampling=0.15 improves class separation without using <code>class_weight='balanced'</code>.</li>
</ul>




  

  <!-- 📈 数据可视化部分 -->
  <h2>📈 Data Visualization Display</h2>

  <h3>1️⃣ Fraud Volume per Transaction Category</h3>
  <img src="fraud_by_category (1).png" width="700">
  <p class="insight">🔍 Fraudulent activity is most concentrated in “grocery_pos” and “gas_transport” categories.</p>

  <h3>2️⃣ 	Top 10 Cities by Number of Fraudulent Transaction </h3>
  <img src="fraud_by_top10_cities (2).png" width="700" alt="Fraud Rate in Top Cities">
  <p class="insight">🔍The fraudulent transactions were evenly distributed across each city, and there was no particularly high concentration in any specific city.</p>
  
  <h3>3️⃣ 	Distribution of Fraudulent Transaction by Log Transform</h3>
  <img src="distribution of fraud transaction by log transform (2).png" width="700" alt="Distribution of Fraudulent Transaction by Log Transform">
  <p class="insight">🔍 Two fraud “hot-spots” emerge.Most cases cluster at log10 ≈ 2.5-3.1 (≈ $320–1 250), suggesting fraudsters prefer high-profit amounts.A smaller spike at log10 ≈ 1.0-1.3 (≈ $8–20) indicates card-testing micro-charges.Few frauds occur between, so risk versus amount is clearly non-linear—models should bin or split on value.
  </p>
  <h3>4️⃣ Top 10 Jobs by Number of Fraudulent Transaction</h3>
<img src="fraud_by_top10_job.png" width="700" alt="Top 10 Jobs by Fraud Number">
  <p class="insight">🔍 The fraudulent transactions were evenly distributed across each job, and there was no particularly high concentration in any specific job.</p>
  
<h3>5️⃣Fraud vs Non-Fraud by Age Interval</h3>
<img src="Fraud vs Non-Fraud by Age Interval (1).png" width="700" alt="Fraud vs Non-Fraud by Age Interval">
  <p class="insight">🔍Age‐interval comparison indicates fraud frequency increases with age, peaking in the 58–95 age group.

<!-- Data Preprocessing -->
<section id="data-cleaning">
  <h2>📦 Data Preprocessing</h2>
  <ol>
    <li>Removed redundant variables and retained only <code>category</code>, <code>amt</code>, <code>gender</code>, <code>job</code>, <code>is_fraud</code>, and <code>age_at_transaction</code> for testing.</li>
    <li>
      <strong>Categorical Variables encoding</strong>
      <ul>
        <li>Considering that <code>category</code> has a relatively small number of subdivisions, I encoded it by mapping each sub-category to a specific integer.</li>
        <li>For <code>job</code>, I initially considered using <code>LabelEncoder</code>, but its integer encoding implies an ordinal relationship among categories, which is not appropriate for this feature. Therefore, I applied frequency encoding instead.</li>
      </ul>
    </li>
    <li>
      <strong>Numerical Variables encoding</strong>
      <ul>
        <li>Scaled <code>amt</code> and <code>age_at_transaction</code> using <code>StandardScaler</code>， as Logistic Regression is a linear model trained using gradient descent, which is highly sensitive to feature scales. StandardScaler can reshape the cost function to be more symmetric, helping the model train faster and more reliably. Plus, standardscaling the varibales ensures that regularization treats all features equally, improving model generalization and interpretability.</li>
      </ul>
    </li>
  </ol>
</section>

<hr>


  <div id="logistic-regression">
    <h2>📊 Logistic Regression Model Evaluation Report</h2>

<h3>📌 Overview</h3>
<ul>
  <li><strong>Model:</strong> Logistic Regression</li>
  <li><strong>Resampling:</strong> SMOTE (0.1) + RandomUnderSampler (0.15)<br>
    <em>I use a low SMOTE ratio to avoid over-generating synthetic fraud samples. Fraud data is often heterogeneous, and high SMOTE ratios (e.g., 0.5) can create unrealistic combinations of dissimilar fraud cases. This synthetic noise may confuse the model and hurt test recall. A smaller ratio helps the model focus on core fraud patterns, reduces overfitting, and preserves the natural class imbalance—which reflects real-world fraud clustering.</em>
  </li>
  <li><strong>Evaluation Focus:</strong> F<sub>2</sub> score (recall-focused)<br>
    <em>In credit card fraud detection, the cost of missing a fraudulent transaction (false negative) is typically much higher than flagging a legitimate one (false positive). Therefore, I use the F<sub>2</sub> score to prioritize recall over precision during model evaluation.</em>
  </li>
</ul>

<h3>🔍 Threshold Optimization</h3>
<ul>
  <li><strong>Best threshold for F<sub>2</sub>:</strong> 0.8074</li>
  <li><strong>Maximum F<sub>2</sub> score:</strong> 0.4071</li>
</ul>

<h3>📈 Performance Metrics</h3>

<h4>📉 Precision-Recall Curve</h3>
<img src="PR_Curve_lr.png" alt="Precision-Recall Curve">


<h4>📉 ROC Curve</h3>
<img src="ROC_Curve_lr.png" alt="ROC Curve">
</ul>

<h3>📋 Classification Report (at F<sub>2</sub>-optimized threshold)</h3>
<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F2-Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Non-Fraud </td>
      <td>0.998</td>
      <td>0.996</td>
      <td>0.997</td>
      <td>553,574</td>
    </tr>
    <tr>
      <td>Fraud </td>
      <td>0.285</td>
      <td>0.456</td>
      <td>0.351</td>
      <td>2,145</td>
    </tr>
  </tbody>
</table>

<h3>🧮 Confusion Matrix</h3>
<table>
  <thead>
    <tr>
      <th></th>
      <th>Predicted: 0</th>
      <th>Predicted: 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Actual: 0</td>
      <td>TN = 551,120</td>
      <td>FP = 2,454</td>
    </tr>
    <tr>
      <td>Actual: 1</td>
      <td>FN = 1,167</td>
      <td>TP = 978</td>
    </tr>
  </tbody>
</table>


<h3>📊 Summary</h3>
<p>
  The model demonstrates strong discriminatory ability (ROC AUC: <strong>0.8405</strong>) and moderate fraud recall (<strong>45.6%</strong>) at the F<sub>2</sub>-optimized threshold, but its low precision (<strong>28.5%</strong>) and modest PR AUC (<strong>0.1465</strong>) highlight the challenges of rare-event detection and indicate that further filtering or post-processing would be necessary for reliable production use.
</p>

    <!-- Random Forest -->
<div id="random-forest">
<h2>📊 Random Forest Model Evaluation Report</h2>

<h3>📌 Overview</h3>
<ul>
  <li><strong>Model:</strong> Random Forest</li>
  <li><strong>Resampling:</strong> SMOTE (0.01) + RandomUnderSampler (0.2)<br>
    <em>I use a low SMOTE ratio to avoid over-generating synthetic fraud samples. Fraud data is often heterogeneous, and high SMOTE ratios can introduce unrealistic patterns that reduce model generalization. A small ratio preserves natural class imbalance and helps focus on core fraud patterns.</em>
  </li>
  <li><strong>Evaluation Focus:</strong> F<sub>2</sub> score (recall-focused)<br>
    <em>In credit card fraud detection, false negatives are more costly than false positives. Therefore, F<sub>2</sub> is used to prioritize recall over precision.</em>
  </li>
</ul>

<h3>🔍 Threshold Optimization</h3>
<ul>
  <li><strong>Best threshold for F<sub>2</sub>:</strong> 0.8500</li>
  <li><strong>Maximum F<sub>2</sub> score:</strong> 0.7042</li>

</ul>

<h3>📈 Performance Metrics</h3>
<ul>
<h4>📉 Precision-Recall Curve</h3>
<img src="PR_Curve_rf.png" alt="Random Forest PR Curve" style="max-width:90%; margin-bottom:20px;">

<h4>📉 ROC Curve</h3>
<img src="ROC_Curve_rf.png" alt="Random Forest ROC Curve" style="max-width:90%; margin-bottom:20px;">
  
<li><strong>OOB Score (on resampled train):</strong> 0.9810</li>
</ul>

<h3>📋 Classification Report (at F<sub>2</sub>-optimized threshold)</h3>
<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1-Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Non-Fraud (0)</td>
      <td>0.999</td>
      <td>0.997</td>
      <td>0.998</td>
      <td>553,574</td>
    </tr>
    <tr>
      <td>Fraud (1)</td>
      <td>0.501</td>
      <td>0.782</td>
      <td>0.611</td>
      <td>2,145</td>
    </tr>
  </tbody>
</table>

<h3>🧮 Confusion Matrix</h3>
<table>
  <thead>
    <tr>
      <th></th>
      <th>Predicted: 0</th>
      <th>Predicted: 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Actual: 0</td>
      <td>TN = 551,904</td>
      <td>FP = 1,670</td>
    </tr>
    <tr>
      <td>Actual: 1</td>
      <td>FN = 468</td>
      <td>TP = 1,677</td>
    </tr>
  </tbody>
</table>
  
<h3>🧠 Feature Importance Analysis</h3>

<h4>🌲 Gini Importance</h4>
<p>
  <img src="Feature_Importance_rf.png" alt="Feature Importance" style="max-width:90%; margin-bottom:20px;">
  <strong>What it measures:</strong> How frequently a feature is used in the decision trees and how much it reduces node impurity (Gini index). Higher values indicate more frequent and effective splits.<br>
  The Gini-based ranking shows <code>amt</code> as the dominant feature (~0.7), followed by <code>category_Code</code> (~0.18). Other features such as <code>age_scaled</code> and <code>job_freq</code> contribute minimally to the tree structure.
</p>

<h4>🧪 Permutation Importance</h4>
<p>
  <img src="Permutation_Importance_rf.png" alt="Permutation Importance" style="max-width:90%; margin-bottom:20px;">
  <strong>What it measures:</strong> How much a feature contributes to the model's predictive performance (e.g., F<sub>2</sub> score), by observing performance drops when the feature is randomly shuffled.<br>
  The permutation results confirm that both <code>amt</code> and <code>category_Code</code> are crucial to the model. Their removal causes a significant decline in F<sub>2</sub>, indicating strong real-world importance.
</p>

<h4>🔍 SHAP Waterfall Plots (Top 3 Fraud Cases)</h4>
<p>
  <div style="display: flex; flex-wrap: wrap; justify-content: space-between;">
    <img src="SHAP_waterfall_rf1.png" alt="SHAP Waterfall Plot 1" style="width: 32%; margin-bottom:20px;">
    <img src="SHAP_waterfall_rf2.png" alt="SHAP Waterfall Plot 2" style="width: 32%; margin-bottom:20px;">
    <img src="SHAP_waterfall_rf3.png" alt="SHAP Waterfall Plot 3" style="width: 32%; margin-bottom:20px;">
  </div>
  <strong>What it measures:</strong> The individual contribution of each feature to a specific prediction. SHAP uses game theory to calculate how each feature pushes the prediction higher or lower.<br>
  These plots highlight how <code>amt</code> and <code>category_Code</code> consistently act as strong positive contributors to fraud predictions in all three cases.
</p>

<strong>Conclusion:</strong> <code>amt</code> and <code>category_Code</code> are consistently the most influential features across Gini, permutation, and SHAP explanations. While Gini reflects tree structure, permutation ties directly to predictive performance, and SHAP gives the most detailed per-instance explanation.</p>

  
<h3>📊 Summary</h3>
<p>
  The Random Forest model achieves excellent ROC AUC (0.9902) and strong PR AUC (0.5914), showing effective separation of fraud and non-fraud transactions even under class imbalance. At the F<sub>2</sub>-optimized threshold of 0.85, the model captures 78.2% of fraud cases with 50.1% precision, reflecting a solid recall-focused trade-off. The out-of-bag (OOB) score on the resampled training set is 0.9810, indicating good model generalization. However, the relatively low precision highlights the need for further filtering or downstream verification in production use.
</p>


<h2>📦 Cost-Based Evaluation of Fraud Detection Model</h2>

<p>
To evaluate the cost performance of the model, I first defined a cost ratio between false negatives (FN) and false positives (FP) as <strong>50:1</strong>. 
In this case, each FN was assigned a cost of <strong>$500</strong>, and each FP a cost of <strong>$10</strong>. 
These values reflect the real-world impact of fraud detection, where failing to detect a fraud (FN) is far more costly than mistakenly flagging a normal transaction (FP).
</p>

<p>
This cost structure is consistent with findings from prior research. 
For example, <a href="https://doi.org/10.1016/j.patcog.2015.03.014" target="_blank"><strong>Dal Pozzolo et al. (2015)</strong></a> noted that while false positives are frequent in credit card fraud detection, their financial impact is minor compared to that of false negatives, which often lead to direct monetary loss. 
Similarly, <a href="https://pages.cs.wisc.edu/~shavlik/cs760/papers/elkan.pdf" target="_blank"><strong>Elkan (2001)</strong></a> emphasized that in real-world classification problems, misclassification costs are rarely equal and often differ by an order of magnitude or more, making cost-sensitive learning essential. 
<a href="https://doi.org/10.1016/j.ins.2019.12.080" target="_blank"><strong>Carcillo et al. (2018)</strong></a> also applied a cost matrix where false negatives were penalized 100 times more than false positives, showing improved business-aligned performance when optimizing for total cost rather than accuracy alone.
</p>

<h3>📚 References (APA Format)</h3>
<ul>
  <li>
    <a href="https://doi.org/10.1016/j.patcog.2015.03.014" target="_blank">
      Dal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2015). 
      <em>Credit card fraud detection: A realistic modeling and a novel learning strategy.</em> 
      Pattern Recognition, 48(10), 3151–3160.
    </a>
  </li>
  <li>
    <a href="https://pages.cs.wisc.edu/~shavlik/cs760/papers/elkan.pdf" target="_blank">
      Elkan, C. (2001). 
      <em>The foundations of cost-sensitive learning.</em> 
      Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI), 973–978.
    </a>
  </li>
  <li>
    <a href="https://doi.org/10.1016/j.ins.2019.12.080" target="_blank">
      Carcillo, F., Dal Pozzolo, A., Le Borgne, Y. A., Caelen, O., Mazzer, Y., & Bontempi, G. (2018). 
      <em>Combining unsupervised and supervised learning in credit card fraud detection.</em> 
      Information Sciences, 557, 317–331.
    </a>
  </li>
</ul>


  <img src="Cost_analysis_lr.png" width="700" alt="Logistic Regression model cost graph">
  
  <p><strong>Note:</strong> <h3>💰 Cost-Based Threshold Optimization (Logistic Regression)</h3>
<p>
Using a predefined cost matrix that assigns greater penalties to missed frauds (false negatives) than to false alerts (false positives), 
I calculated the total cost of classification errors across different decision thresholds on the test set. 
The graph above shows how total cost varies with the probability threshold in the logistic regression model.
</p>

<p>
At low thresholds (between <strong>0.0</strong> and <strong>0.2</strong>), the model predicts too many transactions as fraud, 
resulting in a surge of false positives and keeping the total cost above <strong>$5 million</strong>. 
As the threshold increases, the cost declines sharply, reaching its minimum at a threshold of <strong>0.27</strong>. 
This threshold provides the best trade-off between false positives and false negatives, 
minimizing the overall financial impact of prediction errors.
</p>

<p>
Beyond <strong>0.27</strong>, the total cost begins to rise again, but more gradually, staying under <strong>$2 million</strong>. 
The lowest total cost achieved is approximately <strong>$411,400</strong>, suggesting that setting the threshold near <strong>0.27</strong> 
delivers the most cost-effective fraud detection performance.
</p>


  </div>
  <img src="Cost_analysis_rf.png" width="700" alt="Logistic Regression model cost graph">  
  <p><strong>Note:</strong> <h3>💰 Cost-Based Threshold Optimization (Random Forest)</h3>
<p>
Using a cost-sensitive evaluation approach, I calculated the total financial cost of misclassification across various decision thresholds in the random forest model. 
The cost function heavily penalizes false negatives (missed fraud) over false positives (false alarms), reflecting real-world priorities in fraud detection.
</p>

<p>
As shown in the graph above, total cost is extremely high at thresholds near <strong>0.0</strong> due to an overwhelming number of false positives. 
As the threshold increases, the cost drops significantly and stabilizes between <strong>0.2</strong> and <strong>0.7</strong>. 
The lowest total cost is observed at a threshold of <strong>0.46</strong>, where the model achieves the optimal trade-off between fraud detection and false alerts.
</p>

<p>
At this threshold, the minimum cost is approximately <strong>$186,480</strong>, representing the most cost-effective configuration for the random forest model on the test set. 
Beyond this point, the cost begins to rise again gradually, indicating a decline in fraud detection effectiveness.
</p>


<h2>✅ Evaluating the Hidden Cost of Manual Review in Fraud Detection</h2>

<p>
While evaluating model performance through false negatives (FN) and false positives (FP) is essential, real-world deployment also demands attention to the hidden operational costs of <strong>manual reviews</strong>. 
In fraud detection systems, transactions flagged by the model as suspicious are often routed for human review before a final decision is made. 
This process, although necessary, introduces labor costs that can quickly accumulate at scale.
</p>
  
<p>
<a href="https://www.mastercard.com/us/en/news-and-trends/Insights/2024/manual-fraud-review-cost.html" target="_blank"><strong>Mastercard (2024)</strong></a> highlights that inefficient manual review processes not only increase operational expenses but also erode customer experience due to delays and friction in legitimate transactions. 
Automating these workflows is emphasized as a way to improve internal efficiency and reduce fraud losses without adding unnecessary overhead.
</p>

<p>
<a href="https://www.ravelin.com/blog/are-you-spending-too-much-time-on-manual-review" target="_blank"><strong>Ravelin (2024)</strong></a> further supports this by stating that manual review is one of the most overlooked yet significant cost components in fraud operations. 
According to the MRC Global Fraud Survey, each manual review takes on average <strong>5.6 minutes</strong>, and this time cost—when multiplied across thousands of cases—makes manual review 
<q>the fraud challenge of greatest concern to e-commerce merchants</q>.
</p>

<p>
Together, these insights reinforce that beyond misclassification costs (e.g., <code>$500</code> for a false negative, <code>$10</code> for a false positive), 
<strong>manual review cost is a major factor</strong> in total cost evaluation. 
Ignoring it can lead to underestimating the true cost of a deployed fraud detection model.
</p>

<h3>📚 References</h3>
<ul>
  <li>
    <a href="https://www.mastercard.com/us/en/news-and-trends/Insights/2024/manual-fraud-review-cost.html" target="_blank">
      Mastercard. (2024, May 8). <em>What do inefficient manual fraud reviews really cost you?</em>
    </a>
  </li>
  <li>
    <a href="https://www.ravelin.com/blog/are-you-spending-too-much-time-on-manual-review" target="_blank">
      Ravelin. (2024). <em>Are you spending too much time on manual review?</em>
    </a>
  </li>
</ul>




  
  <!-- 👤 作者 -->
  <div class="footer">
    Created by Andy Wang · Published with GitHub Pages
  </div>

</body>
</html>


