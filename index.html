<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Fraud Detection EDA Report</title>
</head>
<body>
  <h1>ğŸ“Š Fraud Detection Data Analysis</h1>
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Fraud Detection EDA Dashboard</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
      color: #333;
      background-color: #f9f9f9;
    }
    h1 {
      text-align: center;
      color: #1a73e8;
    }
    h2 {
      margin-top: 40px;
      color: #2c3e50;
      border-bottom: 2px solid #ccc;
      padding-bottom: 5px;
    }
    h3 {
      margin-top: 25px;
      color: #444;
    }
    a {
      color: #1a73e8;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      display: block;
      margin: 10px auto 30px;
      border: 1px solid #ccc;
      box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
    }
    p {
      max-width: 800px;
      margin: 0 auto 15px;
      text-align: justify;
    }
    hr {
      margin: 40px 0;
      border: none;
      border-top: 1px solid #ddd;
    }
    .footer {
      text-align: center;
      margin-top: 60px;
      font-style: italic;
      color: #777;
    }
  </style>
</head>
<body>

<h2>ğŸ“¦ Data Resource</h2>
<p class="left-aligned">
  Original dataset from
  <a href="https://www.kaggle.com/kartik2112/fraud-detection" target="_blank">
    Kaggle: Credit Card Fraud Detection
  </a>
</p>

<h2>ğŸ“„ Interactive EDA Report</h2>
<p class="left-aligned">
  Access the full exploratory data analysis report here:
  <a href="fraud_eda_report.html" target="_blank">ğŸ“„ View Full EDA Report (HTML)</a>
</p>

  <!-- ğŸ“ˆ æ•°æ®å¯è§†åŒ–éƒ¨åˆ† -->
  <h2>ğŸ“ˆ Data Visualization Display</h2>

  <h3>1ï¸âƒ£ Fraud Volume per Transaction Category</h3>
  <img src="fraud_by_category (1).png" width="700">
  <p class="insight">ğŸ” Fraudulent activity is most concentrated in â€œgrocery_posâ€ and â€œgas_transportâ€ categories.</p>

  <h3>2ï¸âƒ£ 	Top 10 Cities by Fraud Number </h3>
  <img src="fraud_by_top10_cities.png" width="700" alt="Fraud Rate in Top Cities">
  <p class="insight">ğŸ” The fraud number of these 10 cities are relatively close to each other, among which Birmingham has the highest fraud rate.</p>
  
  <h3>3ï¸âƒ£ 	Distribution of Fraud Amounts vs Normal Distribution</h3>
  <img src="normal_distribution_of_fraud.png" width="700" alt="Fraud Transaction Amount Distribution">
  <p class="insight">ğŸ” This chart compares the distribution of transaction amounts for fraudulent transactions (blue histogram) with a fitted normal distribution (red dashed line). It helps assess how well fraud transaction amounts follow a normal distribution and reveals patterns or anomalies such as skewness or irregular peaks.
  </p>
  <h3>4ï¸âƒ£ Top 10 Jobs by Fraud Number</h3>
<img src="fraud_by_top10_job.png" width="700" alt="Top 10 Jobs by Fraud Number">
  <p class="insight">ğŸ” The job distribution bar chart shows that film/video editors have the highest number of fraudulent transactions
  </p>
<h3>5ï¸âƒ£Fraud vs Non-Fraud by Age Interval</h3>
<img src="Fraud vs Non-Fraud by Age Interval (1).png" width="700" alt="Fraud vs Non-Fraud by Age Interval">
  <p class="insight">ğŸ”Ageâ€interval comparison indicates fraud frequency increases with age, peaking in the 58â€“95 age group.
    
<!-- Data Cleaning -->
<section id="data-cleaning">
  <h2>ğŸ“¦ Data Cleaning</h2>
  <ol>
    <li>Removed redundant variables and retained only <code>category</code>, <code>amt</code>, <code>gender</code>, <code>job</code>, <code>is_fraud</code>, and <code>age_at_transaction</code> for testing.</li>
    <li>
      <strong>Categorical Variables encoding</strong>
      <ul>
        <li>Considering that <code>category</code> has a relatively small number of subdivisions, I encoded it by mapping each sub-category to a specific integer.</li>
        <li>For <code>job</code>, I initially considered using <code>LabelEncoder</code>, but its integer encoding implies an ordinal relationship among categories, which is not appropriate for this feature. Therefore, I applied frequency encoding instead.</li>
      </ul>
    </li>
    <li>
      <strong>Numerical Variables encoding</strong>
      <ul>
        <li>Scaled <code>amt</code> and <code>age_at_transaction</code> using <code>StandardScaler</code>.</li>
      </ul>
    </li>
  </ol>
</section>

<hr>

<!-- Model Building & Testing -->
<section id="model-building">
  <h2>ğŸ¤– Model Building & Testing</h2>

  <div id="logistic-regression">
    <h3>1ï¸âƒ£ Logistic Regression</h3>

    <ol>
      <li>Used all remaining variables to train the model, but the classification report performance was unsatisfactory. 
          <img src="log_reg_class_rep1.jpg" width="700" ></li>
      </p>
      <li>Assessed feature importance by examining the magnitudes of the modelâ€™s coefficients.</li>
      </p>
      <li>Tried optimizing the model by combining Precisionâ€“Recall curve threshold selection with over- and under-sampling. 
         <img src="log_reg_class_rep2.jpg" width="700" ></li>
    </ol>
</p>
    <section id="coef-ci">
  <h3>Confidence Interval of Coefficients</h3>

  <h4>Baseline Fraud Risk (~1.8 %)</h4>
  <p>
    This is the modelâ€™s â€œstarting pointâ€ probability of fraud when all input factors
    (amount, category, gender, job frequency, age) are at their reference level
    (i.e. zero on your scaled/encoded scale). Converting the intercept (â€“3.99) into a
    probability gives about <strong>1.8 %</strong>, meaning that without any other information,
    the model would guess just under a 2 % chance of fraud.
  </p>

  <h4>Confidence Intervals (CIs)</h4>
  <p>
    A 95 % confidence interval is a range around each coefficient that shows how precise
    that estimate is. If the range doesnâ€™t cross zero, weâ€™re reasonably sure (95 % confident)
    that the real effect isnâ€™t just random noise. In my results, all intervals stayed
    either entirely above or entirely below zero, so each factor has a real impact on fraud risk.
  </p>

  <h4>Key Feature Effects</h4>
  <ul>
    <li>
      <strong>Transaction Amount (amt):</strong>
      Higher amounts increase fraud risk. Every one-unit jump in your scaled amount
      raises the odds of fraud by about <strong>58 %</strong>.
    </li>
    <li>
      <strong>Merchant Category (category_Code):</strong>
      Some categories are slightly safer. Moving up one code reduces fraud odds by
      about <strong>15 %</strong>.
    </li>
    <li>
      <strong>Gender (gender_le):</strong>
      Depending on how you coded it, going from 0â†’1 increases fraud odds by roughly
      <strong>23 %</strong>.
    </li>
    <li>
      <strong>Job Frequency (job_freq):</strong>
      This feature nearly guarantees non-fraud in your dataâ€”higher values push fraud
      probability toward zero.
    </li>
    <li>
      <strong>Age (age_scaled):</strong>
      Older customers have a slightly higher fraud risk, with each unit of scaled age
      adding about <strong>16 %</strong> to the odds.
    </li>
  </ul>
</section>

    <h3>ğŸ“Š Key Takeaways</h3>

    </h4>Reading Materials:</strong><h4>
      <li><a href="https://builtin.com/data-science/roc-curves-auc" target="_blank">ROC Curves & AUC</a></li>
      <li><a href="https://builtin.com/data-science/precision-and-recall" target="_blank">Precision and Recall</a></li>
    </ul>

    <h4>What Is an ROC Curve and AUC?</h4>
    <p>An ROC curve (receiver operating characteristic curve) measures a classifierâ€™s performance by plotting the true positive rate against the false positive rate at various thresholds. AUC (area under the ROC curve) represents the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.</p>

    <h4>Why Choose the ROC Curve?</h4>
    <p>AUC is <strong>threshold-invariant</strong> (the metricâ€™s value does not depend on a particular decision threshold) and <strong>scale-invariant</strong> (the metric evaluates ranking quality rather than absolute score values).</p>

    <h4>Trade-off Between TPR and FPR</h4>
    <p>If your goal is to identify as many true positives as possible, you pursue a high TPR, which may come at the cost of a higher FPRâ€”meaning false positives become more frequent. And vice versa.</p>

    <h4>Personal Insights</h4>
    <ul>
      <li><strong>TPR</strong> indicates the proportion of actual positives that the model correctly predicts as positive. <strong>FPR</strong> indicates the proportion of actual negatives that the model incorrectly predicts as positive.</li>
      <li><strong>Precision</strong>: Of all samples predicted positive (TP + FP), what fraction are truly positive (TP).</li>
      <li><strong>Recall</strong>: Of all actual positive samples (TP + FN), what fraction did the model correctly predict as positive (TP).</li>
    </ul>

    
    <h3>ğŸ”‘ Evaluating the Logistic Regression Model: Confusion Matrix, ROC & AUC, and PR Curve</h3>

    <h4>Confusion Matrix</h4>
<img src="confusion_metric.jpg" width="700" ></li>

    <h4>ROC Curve</h4>
  <img src="ROC Curve.png" width="700" ></li>
    <p><strong>Key takeaway:</strong> An AUC of 0.84 means that in 84% of random fraudâ€“non-fraud pairs, the fraud transaction gets a higher score. This indicates good general discrimination, but it doesnâ€™t tell you how many false alarms youâ€™ll get at a working threshold.</p>

    <h4>PR Curve</h4>
 <img src="PR Curve.png" width="700" ></li>
    <p><strong>Key takeaway:</strong> An Average Precision of 0.19 means thatâ€”even though you can rank frauds relatively wellâ€”the modelâ€™s practical precision is low: when it flags a transaction as fraud, itâ€™s correct only 19% of the time on average.</p>


    </p>
    <h3>ğŸ“ Comment</h3>
    <p>Although the model achieves strong overall discriminative power (ROC-AUC = 0.84), precision on the minority class remains low (AP = 0.19), so logistic regression is not an effective tool to predict credit card fraud.</p>
  </div>
</section>
  </p>
  <!-- Random Forest -->
<div id="random-forest">
  <h3>2ï¸âƒ£ Random Forest</h3>
    <ol>
      <li>Using the Random Forest with only class_weight='balanced', the model achieved a precision of 0.84 and a recall of 0.69. This configuration prioritized reducing false negatives, only correctly flagging 69% of fraudulent transactions while maintaining a high level of precision.</li>
          <img src="rf_class_rep1.jpg" width="700" ></li>
      <li>Next, I applied both SMOTE (oversampling the minority class at a 0.5 ratio) and random undersampling of the majority class. While this boosted recall to 0.87â€”meaning the vast majority of fraud cases were detectedâ€”it came at the cost of precision dropping to 0.41. In practical terms, many non-fraudulent transactions were now being misclassified as fraud.</li>
                <img src="rf_class_rep2.jpg" width="700" ></li>
      <li>Finally, I tried to remove undersampling and reduce the SMOTE ratio from 0.5 to 0.1. I then used the Precisionâ€“Recall curve to identify the optimal decision threshold for maximizing the F1 score. With this tuned setup, the model struck a better balance, yielding a precision of 0.766 and a recall of 0.723.
         <img src="rf_class_rep3.jpg" width="700" ></li>
    </ol>
  </p>
    <h3>ğŸ“Š Evaluating the Random forest Model: ROC & AUC and PR Curve,</h3>
  
      <h4>PR Curve</h4>
  <img src="PR_Cuver_rf.png" width="350" alt="RF Confusion Matrix">
   <p><strong>Key takeaway:</strong> At very low recall , we can get near-perfect precision, but beyond ~0.7 recall precision dips below 0.8. The red dot marks the threshold (â‰ˆ0.73) that maximizes Fâ‚, giving us about precision â‰ˆ 0.77 and recall â‰ˆ 0.73. Thus, we can strike a balanceâ€”catch roughly 73% of frauds while still being right about 77% of the time.</p>
           
     <h4>ROC Curve</h4>
  <img src="ROC_Curve_rf.png" width="350" alt="RF ROC & AUC">
    <p><strong>Key takeaway:</strong>The ROC curve shoots almost straight up to a true positive rate (TPR) near 1.0 by the time false positive rate (FPR) is only ~0.1, and the area under curve is 0.99. That indicates nearly perfect rank-ordering of fraud vs. non-fraud by the model.</p>


  </p>
    </li>
    <h3>ğŸ“Š Feature Importance</h3>
      <div>
        <h4>1ï¸âƒ£ Gini-based Importance</h4>
        <img src="feature_importance_rf.png" width="350" alt="RF Gini Importance">
        
        <h4>2ï¸âƒ£ Permutation-based Importance</h4>
        <img src="permutation_importance_rf.png" width="350" alt="RF Permutation Importance">
        
    <p><strong>Note:</strong>Both the Giniâ€based and permutationâ€based importance rankings confirm that transaction amount and category are by far the most influential predictors in Random Forest Model, with the other three features contributing only marginally. The Gini scores  somewhat overstate the gap between amt and category_Code, whereas permutation importance reveals theyâ€™re nearly equally critical to model performance. In practice, this means the fraud detector model I built relies almost entirely on those two variables. </p>
    </li>
  </ol>

</p>
  <h3>ğŸ“Š SHAP Analysis ( Fraud Data) </h3>
  <div>
    <img src="SHAP_waterfall1_rf.png" width="700" alt="SHAP Waterfall 1">
  </div>
    <img src="SHAP_waterfall2_rf.png" width="700" alt="SHAP Waterfall 2">
  </div>
    <p><strong> <class="insight">ğŸ” Interpretation of Waterfall Plot:</strong> In each case, the transaction amount (amt) is the single biggest push (+0.52/ +0.53), followed by the category_Code=2 (+0.45/ +0.46). The age_scaled and job_freq features add only a sliver (+0.01 or less), while gender_le slightly suppresses the score. In short, Random Forest flags these two transactions as fraud almost entirely on the basis of an unusually large amount and a highâ€risk categoryâ€”other features barely move the needle.
  
      <h3>ğŸ“ Comment</h3>
  <p> Random Forest achieves nearâ€perfect discriminative power (ROC-AUC = 0.99), its performance on the minority fraud class (precision = 0.766, recall = 0.723) is relatively high enough to detect credit card fraud compared to logistic model.
    
  </p>
</div>




  
  <!-- ğŸ‘¤ ä½œè€… -->
  <div class="footer">
    Created by Andy Wang Â· Published with GitHub Pages
  </div>

</body>
</html>


