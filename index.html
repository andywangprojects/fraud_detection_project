<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Credit Card Transactions Fraud Detection Report</title>
</head>
<body>
  <h1>📊 Credit Card Transaction Fraud Detection Data Analysis</h1>
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Fraud Detection EDA Dashboard</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
      color: #333;
      background-color: #f9f9f9;
    }
    h1 {
      text-align: center;
      color: #1a73e8;
    }
    h2 {
      margin-top: 40px;
      color: #2c3e50;
      border-bottom: 2px solid #ccc;
      padding-bottom: 5px;
    }
    h3 {
      margin-top: 25px;
      color: #444;
    }
    a {
      color: #1a73e8;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      display: block;
      margin: 10px auto 30px;
      border: 1px solid #ccc;
      box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
    }
    p {
      max-width: 800px;
      margin: 0 auto 15px;
      text-align: justify;
    }
    hr {
      margin: 40px 0;
      border: none;
      border-top: 1px solid #ddd;
    }
    .footer {
      text-align: center;
      margin-top: 60px;
      font-style: italic;
      color: #777;
    }
  </style>
</head>
<body>

<h2>📦 Data Resource</h2>
<p class="left-aligned">
  Original dataset from
  <a href="https://www.kaggle.com/kartik2112/fraud-detection" target="_blank">
    Kaggle: Credit Card Fraud Detection
  </a>
</p>

<h2>📄 Interactive EDA Report</h2>
<p class="left-aligned">
  Access the full exploratory data analysis report here:<br>
  <a href="train_eda_report.html" target="_blank">📄 View Full EDA Report for Train Data (HTML)</a>
  <a href="test_eda_report.html" target="_blank">📄 View Full EDA Report for Test Data (HTML)</a>
</p>

 <h2>📊 Class Imbalance Analysis </h2>

<h3>🧾 Distribution of the Data</h3>
<p>
The  dataset contains a total of <strong>1,844,974</strong> credit card transaction records labeled as <code>is_fraud</code>, indicating whether a transaction is fraudulent (1) or not (0).
</p>
<ul>
  <li><strong>Non-Fraudulent transactions (label 0):</strong> 1,835,624 (<strong>99.4941%</strong>)</li>
  <li><strong>Fraudulent transactions (label 1):</strong> 9,350 (<strong>0.5059%</strong>)</li>
</ul>
<p>
This extreme class imbalance poses a significant challenge for classification models. While the dataset is dominated by legitimate transactions, the minority class (fraudulent cases) is of primary importance, especially in practical fraud detection systems. 
</p>
  
<h4>⚠️ Why Tackling Imbalanced Data is Important</h3>
<p>
For a classification model, especially in domains such as fraud detection, medical diagnostics, and spam filtering, rare but high-impact cases are often the primary concern. However, if imbalanced data is not properly addressed during training, the model may appear to perform well—achieving high overall accuracy—while completely failing to detect the minority class. This results in misleading evaluation metrics and significantly reduces the model’s practical value.In such datasets, the minority class often behaves like an outlier due to its sparse distribution in the feature space. Many models interpret these rare patterns as noise or unrepresentative signals, leading them to underfit or ignore the minority class. Since most algorithms aim to minimize overall error, they are naturally biased toward the majority class.Consequently, imbalanced data can cause the model to overfit the dominant class while neglecting the minority class, which—ironically—is often the class of greatest importance. Addressing this imbalance is therefore critical to building models that are not only statistically sound but also effective in real-world decision-making.
</p>

<h3>📏 Metrics & Methods for Imbalanced Data</h3>
<ul>
  <li><strong>SMOTE (Synthetic Minority Over-sampling Technique):</strong> A technique that creates synthetic samples of the minority class by interpolating between existing examples, helping the model learn core patterns without overfitting to duplicates.</li>
  <li><strong>Undersampling:</strong> Randomly removes samples from the majority class to balance the dataset. 
  <li><strong>F<sub>2</sub> Score:</strong> A weighted version of the F1-score that prioritizes <strong>recall</strong> over precision, making it more suitable for fraud detection where missing a fraud is more costly than false alerts.</li>
</ul>



  

  <!-- 📈 数据可视化部分 -->
  <h2>📈 Data Visualization Display</h2>

  <h3>1️⃣ Fraud Volume per Transaction Category</h3>
  <img src="fraud_by_category (1).png" width="700">
  <p class="insight">🔍 Fraudulent activity is most concentrated in “grocery_pos” and “gas_transport” categories.</p>

  <h3>2️⃣ 	Top 10 Cities by Number of Fraudulent Transaction </h3>
  <img src="fraud_by_top10_cities (2).png" width="700" alt="Fraud Rate in Top Cities">
  <p class="insight">🔍The fraudulent transactions were evenly distributed across each city, and there was no particularly high concentration in any specific city.</p>
  
  <h3>3️⃣ 	Distribution of Fraudulent Transaction by Log Transform</h3>
  <img src="distribution of fraud transaction by log transform (2).png" width="700" alt="Distribution of Fraudulent Transaction by Log Transform">
  <p class="insight">🔍 Two fraud “hot-spots” emerge.Most cases cluster at log10 ≈ 2.5-3.1 (≈ $320–1 250), suggesting fraudsters prefer high-profit amounts.A smaller spike at log10 ≈ 1.0-1.3 (≈ $8–20) indicates card-testing micro-charges.Few frauds occur between, so risk versus amount is clearly non-linear—models should bin or split on value.
  </p>
  <h3>4️⃣ Top 10 Jobs by Number of Fraudulent Transaction</h3>
<img src="fraud_by_top10_job.png" width="700" alt="Top 10 Jobs by Fraud Number">
  <p class="insight">🔍 The fraudulent transactions were evenly distributed across each job, and there was no particularly high concentration in any specific job.</p>
  
<h3>5️⃣Fraud vs Non-Fraud by Age Interval</h3>
<img src="Fraud vs Non-Fraud by Age Interval (1).png" width="700" alt="Fraud vs Non-Fraud by Age Interval">
  <p class="insight">🔍Age‐interval comparison indicates fraud frequency increases with age, peaking in the 58–95 age group.

<!-- Data Preprocessing -->
<section id="data-cleaning">
  <h2>📦 Data Preprocessing</h2>
  <ol>
    <li>Removed redundant variables and retained only <code>category</code>, <code>amt</code>, <code>gender</code>, <code>job</code>, <code>is_fraud</code>, and <code>age_at_transaction</code> for testing.</li>
    <li>
      <strong>Categorical Variables encoding</strong>
      <ul>
        <li>Considering that <code>category</code> has a relatively small number of subdivisions, I encoded it by mapping each sub-category to a specific integer.</li>
        <li>For <code>job</code>, I initially considered using <code>LabelEncoder</code>, but its integer encoding implies an ordinal relationship among categories, which is not appropriate for this feature. Therefore, I applied frequency encoding instead.</li>
      </ul>
    </li>
    <li>
      <strong>Numerical Variables encoding</strong>
      <ul>
        <li>Scaled <code>amt</code> and <code>age_at_transaction</code> using <code>StandardScaler</code>， as Logistic Regression is a linear model trained using gradient descent, which is highly sensitive to feature scales. StandardScaler can reshape the cost function to be more symmetric, helping the model train faster and more reliably. Plus, standardscaling the varibales ensures that regularization treats all features equally, improving model generalization and interpretability.</li>
      </ul>
    </li>
  </ol>
</section>

<hr>


  <div id="logistic-regression">
    <h2>📊 Logistic Regression Model Evaluation Report</h2>

<h3>📌 Overview</h3>
<ul>
  <li><strong>Model:</strong> Logistic Regression</li>
  <li><strong>Resampling:</strong> SMOTE (10%) + RandomUnderSampler (balanced)<br>
    <em>I use a low SMOTE ratio of 10% to avoid over-generating synthetic fraud samples. Fraud data is often heterogeneous, and high SMOTE ratios (e.g., 0.5) can create unrealistic combinations of dissimilar fraud cases. This synthetic noise may confuse the model and hurt test recall. A smaller ratio helps the model focus on core fraud patterns, reduces overfitting, and preserves the natural class imbalance—which reflects real-world fraud clustering.</em>
  </li>
  <li><strong>Evaluation Focus:</strong> F<sub>2</sub> score (recall-focused)<br>
    <em>In credit card fraud detection, the cost of missing a fraudulent transaction (false negative) is typically much higher than flagging a legitimate one (false positive). Therefore, I use the F<sub>2</sub> score to prioritize recall over precision during model evaluation.</em>
  </li>
</ul>

<h3>🔍 Threshold Optimization</h3>
<ul>
  <li><strong>Best threshold for F<sub>2</sub>:</strong> 0.9758</li>
  <li><strong>Maximum F<sub>2</sub> score:</strong> 0.4091</li>
</ul>

<h3>📈 Performance Metrics</h3>

<h4>📉 Precision-Recall Curve</h3>
<img src="PR_Curve_lr.png" alt="Precision-Recall Curve">


<h4>📉 ROC Curve</h3>
<img src="ROC_Curve_lr.png" alt="ROC Curve">
</ul>

<h3>📋 Classification Report (at F<sub>2</sub>-optimized threshold)</h3>
<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F2-Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Non-Fraud </td>
      <td>0.998</td>
      <td>0.996</td>
      <td>0.997</td>
      <td>553,574</td>
    </tr>
    <tr>
      <td>Fraud </td>
      <td>0.287</td>
      <td>0.458</td>
      <td>0.353</td>
      <td>2,145</td>
    </tr>
  </tbody>
</table>

<h3>🧮 Confusion Matrix</h3>
<table>
  <thead>
    <tr>
      <th></th>
      <th>Predicted: 0</th>
      <th>Predicted: 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Actual: 0</td>
      <td>TN = 551,132</td>
      <td>FP = 2,442</td>
    </tr>
    <tr>
      <td>Actual: 1</td>
      <td>FN = 1,163</td>
      <td>TP = 982</td>
    </tr>
  </tbody>
</table>


<h3>📊 Summary</h3>
<p>
  The model demonstrates strong discriminatory ability (ROC AUC: <strong>0.8341</strong>) and moderate fraud recall (<strong>45.8%</strong>) at the F<sub>2</sub>-optimized threshold, but its low precision (<strong>28.7%</strong>) and modest PR AUC (<strong>0.1401</strong>) highlight the challenges of rare-event detection and indicate that further filtering or post-processing would be necessary for reliable production use.
</p>

    <!-- Random Forest -->
<div id="random-forest">
<h2>📊 Random Forest Model Evaluation Report</h2>

<h3>📌 Overview</h3>
<ul>
  <li><strong>Model:</strong> Random Forest</li>
  <li><strong>Resampling:</strong> SMOTE (10%) + RandomUnderSampler (balanced)<br>
    <em>I use a low SMOTE ratio of 10% to avoid over-generating synthetic fraud samples. Fraud data is often heterogeneous, and high SMOTE ratios can introduce unrealistic patterns that reduce model generalization. A small ratio preserves natural class imbalance and helps focus on core fraud patterns.</em>
  </li>
  <li><strong>Evaluation Focus:</strong> F<sub>2</sub> score (recall-focused)<br>
    <em>In credit card fraud detection, false negatives are more costly than false positives. Therefore, F<sub>2</sub> is used to prioritize recall over precision.</em>
  </li>
</ul>

<h3>🔍 Threshold Optimization</h3>
<ul>
  <li><strong>Best threshold for F<sub>2</sub>:</strong> 0.7800</li>
  <li><strong>Maximum F<sub>2</sub> score:</strong> 0.6433</li>
  <li><strong>F<sub>2</sub> score at threshold:</strong> 0.6395</li>
</ul>

<h3>📈 Performance Metrics</h3>
<ul>
<h4>📉 Precision-Recall Curve</h3>
<img src="PR_Curve_rf.png" alt="Random Forest PR Curve" style="max-width:90%; margin-bottom:20px;">

<h4>📉 ROC Curve</h3>
<img src="ROC_Curve_rf.png" alt="Random Forest ROC Curve" style="max-width:90%; margin-bottom:20px;">
  
<li><strong>OOB Score (on resampled train):</strong> 0.9948</li>
</ul>

<h3>📋 Classification Report (at F<sub>2</sub>-optimized threshold)</h3>
<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1-Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Non-Fraud (0)</td>
      <td>0.999</td>
      <td>0.997</td>
      <td>0.998</td>
      <td>553,574</td>
    </tr>
    <tr>
      <td>Fraud (1)</td>
      <td>0.472</td>
      <td>0.702</td>
      <td>0.565</td>
      <td>2,145</td>
    </tr>
  </tbody>
</table>

<h3>🧮 Confusion Matrix</h3>
<table>
  <thead>
    <tr>
      <th></th>
      <th>Predicted: 0</th>
      <th>Predicted: 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Actual: 0</td>
      <td>TN = 551,892</td>
      <td>FP = 1,682</td>
    </tr>
    <tr>
      <td>Actual: 1</td>
      <td>FN = 640</td>
      <td>TP = 1,505</td>
    </tr>
  </tbody>
</table>
  
<h3>🧠 Feature Importance Analysis</h3>

<h4>🌲 Gini Importance</h4>
<p>
  <img src="Feature_Importance_rf.png" alt="Feature Importance" style="max-width:90%; margin-bottom:20px;">
  <strong>What it measures:</strong> How frequently a feature is used in the decision trees and how much it reduces node impurity (Gini index). Higher values indicate more frequent and effective splits.<br>
  The Gini-based ranking shows <code>amt</code> as the dominant feature (~0.7), followed by <code>category_Code</code> (~0.18). Other features such as <code>age_scaled</code> and <code>job_freq</code> contribute minimally to the tree structure.
</p>

<h4>🧪 Permutation Importance</h4>
<p>
  <img src="Permutation_Importance_rf.png" alt="Permutation Importance" style="max-width:90%; margin-bottom:20px;">
  <strong>What it measures:</strong> How much a feature contributes to the model's predictive performance (e.g., F<sub>2</sub> score), by observing performance drops when the feature is randomly shuffled.<br>
  The permutation results confirm that both <code>amt</code> and <code>category_Code</code> are crucial to the model. Their removal causes a significant decline in F<sub>2</sub>, indicating strong real-world importance.
</p>

<h4>🔍 SHAP Waterfall Plots (Top 3 Fraud Cases)</h4>
<p>
  <div style="display: flex; flex-wrap: wrap; justify-content: space-between;">
    <img src="SHAP_waterfall_rf1.png" alt="SHAP Waterfall Plot 1" style="width: 32%; margin-bottom:20px;">
    <img src="SHAP_waterfall_rf2.png" alt="SHAP Waterfall Plot 2" style="width: 32%; margin-bottom:20px;">
    <img src="SHAP_waterfall_rf3.png" alt="SHAP Waterfall Plot 3" style="width: 32%; margin-bottom:20px;">
  </div>
  <strong>What it measures:</strong> The individual contribution of each feature to a specific prediction. SHAP uses game theory to calculate how each feature pushes the prediction higher or lower.<br>
  These plots highlight how <code>amt</code> and <code>category_Code</code> consistently act as strong positive contributors to fraud predictions in all three cases.
</p>

<strong>Conclusion:</strong> <code>amt</code> and <code>category_Code</code> are consistently the most influential features across Gini, permutation, and SHAP explanations. While Gini reflects tree structure, permutation ties directly to predictive performance, and SHAP gives the most detailed per-instance explanation.</p>

  
<h3>📊 Summary</h3>
<p>
  The Random Forest model achieves excellent ROC AUC (0.9783) and strong PR AUC (0.6044), showing effective separation of fraud and non-fraud transactions even under class imbalance. At the F<sub>2</sub>-optimized threshold of 0.78, the model captures 70.2% of fraud cases with 47.2% precision, reflecting a solid recall-focused trade-off. The out-of-bag (OOB) score on the resampled training set is 0.9948, indicating good model generalization. However, the relatively low precision highlights the need for further filtering or downstream verification in production use.
</p>

  <h2>📦 Cost Evaluation</h2>
To evaluate the cost performance of the model, I first defined a cost ratio between false negatives (FN) and false positives (FP) as 50:1. In this case, each FN was assigned a cost of $500, and each FP a cost of $10. These values reflect the real-world impact of fraud detection, where failing to detect a fraud (FN) is far more costly than mistakenly flagging a normal transaction (FP).
  <img src="Cost_Sensitivity_lg.png" width="700" alt="Logistic Regression model cost graph">
  
  <p><strong>Note:</strong>  Using these cost values, I calculated the total cost for different decision thresholds on the test set. The graph above shows how total cost changes as the probability threshold varies in the logistic model. It is clear that when the threshold is between 0.0 and 0.2, the total cost stays extremely high, around $3.8 million. This is likely because, at low thresholds, the model predicts too many positives, increasing the number of false positives.As the threshold increases, the total cost drops sharply, reaching the minimum point at a threshold of 0.65. This threshold represents the best trade-off between FPs and FNs, resulting in the lowest total cost. After this point, the total cost begins to rise again, but at a slower rate, remaining below $1.5 million. This suggests that the model performs best when the threshold is set around 0.65, with the minimum cost of $0.38 million helping reduce overall losses from fraud detection errors.<p>
  </div>
  <img src="Cost_Sensitivity_rf.png" width="700" alt="Logistic Regression model cost graph">  
  <p><strong>Note:</strong>  Unlike the logistic regression model, the cost curve for the random forest model appears much smoother, with less fluctuation. As the probability threshold increases, the total cost slightly decreases at first, reaching its minimum at a very low threshold of 0.08. At this point, the minimum total cost is approximately $130,000.
After this optimal threshold, the total cost starts to rise steadily. The cost reaches its highest point when the threshold is close to 1.0. This increase happens because, as the threshold becomes higher, fewer transactions are flagged as fraudulent. As a result, more fraud cases are missed, leading to a growing number of false negatives, which are far more costly in this scenario.
Overall, the graph suggests that the random forest model performs best at a low threshold, around 0.08, where it can catch more fraud cases early and minimize total cost. This highlights the model’s strength in detecting fraud even with relatively low prediction probabilities.</p>
    





  
  <!-- 👤 作者 -->
  <div class="footer">
    Created by Andy Wang · Published with GitHub Pages
  </div>

</body>
</html>


