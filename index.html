<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Credit Card Transactions Fraud Detection Report</title>
</head>
<body>
  <h1>ğŸ“Š Credit Card Transaction Fraud Detection Data Analysis</h1>
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Fraud Detection EDA Dashboard</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
      color: #333;
      background-color: #f9f9f9;
    }
    h1 {
      text-align: center;
      color: #1a73e8;
    }
    h2 {
      margin-top: 40px;
      color: #2c3e50;
      border-bottom: 2px solid #ccc;
      padding-bottom: 5px;
    }
    h3 {
      margin-top: 25px;
      color: #444;
    }
    a {
      color: #1a73e8;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      display: block;
      margin: 10px auto 30px;
      border: 1px solid #ccc;
      box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
    }
    p {
      max-width: 800px;
      margin: 0 auto 15px;
      text-align: justify;
    }
    hr {
      margin: 40px 0;
      border: none;
      border-top: 1px solid #ddd;
    }
    .footer {
      text-align: center;
      margin-top: 60px;
      font-style: italic;
      color: #777;
    }
  </style>
</head>
<body>

<h2>ğŸ“¦ Data Resource</h2>
<p class="left-aligned">
  Original dataset from
  <a href="https://www.kaggle.com/kartik2112/fraud-detection" target="_blank">
    Kaggle: Credit Card Fraud Detection
  </a>
</p>

<h2>ğŸ“„ Interactive EDA Report</h2>
<p class="left-aligned">
  Access the full exploratory data analysis report here:<br>
  <a href="train_eda_report.html" target="_blank">ğŸ“„ View Full EDA Report for Train Data (HTML)</a>
  <a href="test_eda_report.html" target="_blank">ğŸ“„ View Full EDA Report for Test Data (HTML)</a>
</p>

 <h2>ğŸ“Š Class Imbalance Analysis </h2>

<h3>ğŸ§¾ Distribution of the data</h3>
The training dataset contains a total of <strong>1,844,974</strong> credit card transaction records labeled as <code>is_fraud</code>, indicating whether a transaction is fraudulent (1) or not (0).</p>
<li><strong>Non-Fraudulent transactions (label 0):</strong> 1,835,624 (<strong>99.4941%</strong>)</li>
<li><strong>Fraudulent transactions (label 1):</strong> 9,350 (<strong>0.5059%</strong>)</li>
</ul>

<h3>ğŸ§¾ Why tackling imbalanced data is important?</h3>
<p>
For a classification model, especially in domains such as fraud detection, medical diagnostics, and spam filtering, rare but high-impact cases are often the primary concern. However, if imbalanced data is not properly addressed during training, the model may appear to perform wellâ€”achieving high overall accuracyâ€”while completely failing to detect the minority class. This results in misleading evaluation metrics and significantly reduces the modelâ€™s practical value.

In such datasets, the minority class often behaves like an outlier due to its sparse distribution in the feature space. Many models interpret these rare patterns as statistical noise or unrepresentative signals, leading them to ignore or underfit the minority class. Since most algorithms are optimized to minimize overall error, they are inherently biased toward the majority class.

Consequently, imbalanced data can cause the model to overfit the dominant class while neglecting the minority class, which, ironically, is typically the class of greatest importance. Addressing this imbalance is therefore critical to building models that are not only statistically sound but also effective in real-world decision-making.
</p>


  

  <!-- ğŸ“ˆ æ•°æ®å¯è§†åŒ–éƒ¨åˆ† -->
  <h2>ğŸ“ˆ Data Visualization Display</h2>

  <h3>1ï¸âƒ£ Fraud Volume per Transaction Category</h3>
  <img src="fraud_by_category (1).png" width="700">
  <p class="insight">ğŸ” Fraudulent activity is most concentrated in â€œgrocery_posâ€ and â€œgas_transportâ€ categories.</p>

  <h3>2ï¸âƒ£ 	Top 10 Cities by Number of Fraudulent Transaction </h3>
  <img src="fraud_by_top10_cities (2).png" width="700" alt="Fraud Rate in Top Cities">
  <p class="insight">ğŸ”The fraudulent transactions were evenly distributed across each city, and there was no particularly high concentration in any specific city.</p>
  
  <h3>3ï¸âƒ£ 	Distribution of Fraudulent Transaction by Log Transform</h3>
  <img src="distribution of fraud transaction by log transform (2).png" width="700" alt="Distribution of Fraudulent Transaction by Log Transform">
  <p class="insight">ğŸ” Two fraud â€œhot-spotsâ€ emerge.Most cases cluster at log10 â‰ˆ 2.5-3.1 (â‰ˆ $320â€“1 250), suggesting fraudsters prefer high-profit amounts.A smaller spike at log10 â‰ˆ 1.0-1.3 (â‰ˆ $8â€“20) indicates card-testing micro-charges.Few frauds occur between, so risk versus amount is clearly non-linearâ€”models should bin or split on value.
  </p>
  <h3>4ï¸âƒ£ Top 10 Jobs by Number of Fraudulent Transaction</h3>
<img src="fraud_by_top10_job.png" width="700" alt="Top 10 Jobs by Fraud Number">
  <p class="insight">ğŸ” The fraudulent transactions were evenly distributed across each job, and there was no particularly high concentration in any specific job.</p>
  
<h3>5ï¸âƒ£Fraud vs Non-Fraud by Age Interval</h3>
<img src="Fraud vs Non-Fraud by Age Interval (1).png" width="700" alt="Fraud vs Non-Fraud by Age Interval">
  <p class="insight">ğŸ”Ageâ€interval comparison indicates fraud frequency increases with age, peaking in the 58â€“95 age group.

<!-- Data Preprocessing -->
<section id="data-cleaning">
  <h2>ğŸ“¦ Data Preprocessing</h2>
  <ol>
    <li>Removed redundant variables and retained only <code>category</code>, <code>amt</code>, <code>gender</code>, <code>job</code>, <code>is_fraud</code>, and <code>age_at_transaction</code> for testing.</li>
    <li>
      <strong>Categorical Variables encoding</strong>
      <ul>
        <li>Considering that <code>category</code> has a relatively small number of subdivisions, I encoded it by mapping each sub-category to a specific integer.</li>
        <li>For <code>job</code>, I initially considered using <code>LabelEncoder</code>, but its integer encoding implies an ordinal relationship among categories, which is not appropriate for this feature. Therefore, I applied frequency encoding instead.</li>
      </ul>
    </li>
    <li>
      <strong>Numerical Variables encoding</strong>
      <ul>
        <li>Scaled <code>amt</code> and <code>age_at_transaction</code> using <code>StandardScaler</code>.</li>
      </ul>
    </li>
  </ol>
</section>

<hr>


  <div id="logistic-regression">
    <h2>ğŸ“Š Logistic Regression Model Evaluation Report</h2>

<h3>ğŸ“Œ Overview</h3>
<ul>
  <li><strong>Model:</strong> Logistic Regression</li>
  <li><strong>Resampling:</strong> SMOTE (10%) + RandomUnderSampler (balanced)<br>
    <em>I use a low SMOTE ratio of 10% to avoid over-generating synthetic fraud samples. Fraud data is often heterogeneous, and high SMOTE ratios (e.g., 0.5) can create unrealistic combinations of dissimilar fraud cases. This synthetic noise may confuse the model and hurt test recall. A smaller ratio helps the model focus on core fraud patterns, reduces overfitting, and preserves the natural class imbalanceâ€”which reflects real-world fraud clustering.</em>
  </li>
  <li><strong>Evaluation Focus:</strong> F<sub>2</sub> score (recall-focused)<br>
    <em>In credit card fraud detection, the cost of missing a fraudulent transaction (false negative) is typically much higher than flagging a legitimate one (false positive). Therefore, I use the F<sub>2</sub> score to prioritize recall over precision during model evaluation.</em>
  </li>
</ul>

<h3>ğŸ” Threshold Optimization</h3>
<ul>
  <li><strong>Best threshold for F<sub>2</sub>:</strong> 0.9758</li>
  <li><strong>Maximum F<sub>2</sub> score:</strong> 0.4091</li>
</ul>

<h3>ğŸ“ˆ Performance Metrics</h3>
  <h3>ğŸ“‰ Precision-Recall Curve</h3>
<img src="PR_Curve_lr.png" alt="Precision-Recall Curve">


<h3>ğŸ“‰ ROC Curve</h3>
<img src="ROC_Curve_lr.png" alt="ROC Curve">
</ul>

<h3>ğŸ“‹ Classification Report (at F<sub>2</sub>-optimized threshold)</h3>
<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F2-Score</th>
      <th>Support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Non-Fraud </td>
      <td>0.998</td>
      <td>0.996</td>
      <td>0.997</td>
      <td>553,574</td>
    </tr>
    <tr>
      <td>Fraud </td>
      <td>0.287</td>
      <td>0.458</td>
      <td>0.353</td>
      <td>2,145</td>
    </tr>
  </tbody>
</table>

<h3>ğŸ§® Confusion Matrix</h3>
<table>
  <thead>
    <tr>
      <th></th>
      <th>Predicted: 0</th>
      <th>Predicted: 1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Actual: 0</td>
      <td>TN = 551,132</td>
      <td>FP = 2,442</td>
    </tr>
    <tr>
      <td>Actual: 1</td>
      <td>FN = 1,163</td>
      <td>TP = 982</td>
    </tr>
  </tbody>
</table>


<h3>ğŸ“Š Summary</h3>
<p>
  The model demonstrates strong discriminatory ability (ROC AUC: <strong>0.8341</strong>) and moderate fraud recall (<strong>45.8%</strong>) at the F<sub>2</sub>-optimized threshold, but its low precision (<strong>28.7%</strong>) and modest PR AUC (<strong>0.1401</strong>) highlight the challenges of rare-event detection and indicate that further filtering or post-processing would be necessary for reliable production use.
</p>

    <!-- Random Forest -->
<div id="random-forest">
  <h3>2ï¸âƒ£ Random Forest</h3>
    <ol>
      <li>Using the Random Forest with only class_weight='balanced', the model achieved a precision of 0.84 and a recall of 0.69. This configuration prioritized reducing false negatives, only correctly flagging 69% of fraudulent transactions while maintaining a high level of precision.</li>
          <img src="rf_class_rep1.jpg" width="700" ></li>
      <li>Next, I applied both SMOTE (oversampling the minority class at a 0.5 ratio) and random undersampling of the majority class. While this boosted recall to 0.87â€”meaning the vast majority of fraud cases were detectedâ€”it came at the cost of precision dropping to 0.41. In practical terms, many non-fraudulent transactions were now being misclassified as fraud.</li>
                <img src="rf_class_rep2.jpg" width="700" ></li>
      <li>Finally, I tried to remove undersampling and reduce the SMOTE ratio from 0.5 to 0.1. I then used the Precisionâ€“Recall curve to identify the optimal decision threshold for maximizing the F1 score. With this tuned setup, the model struck a better balance, yielding a precision of 0.766 and a recall of 0.723.
         <img src="rf_class_rep3.jpg" width="700" ></li>
    </ol>
  </p>
    <h3>ğŸ“Š Evaluating the Random forest Model: ROC & AUC and PR Curve,</h3>
  
      <h4>PR Curve</h4>
  <img src="PR_Cuver_rf.png" width="700" alt="RF Confusion Matrix">
   <p><strong>Key takeaway:</strong> At very low recall , we can get near-perfect precision, but beyond ~0.7 recall precision dips below 0.8. The red dot marks the threshold (â‰ˆ0.73) that maximizes Fâ‚, giving us about precision â‰ˆ 0.77 and recall â‰ˆ 0.73. Thus, we can strike a balanceâ€”catch roughly 73% of frauds while still being right about 77% of the time.</p>
           
     <h4>ROC Curve</h4>
  <img src="ROC_Curve_rf.png" width="700" alt="RF ROC & AUC">
    <p><strong>Key takeaway:</strong> The curve almost hugs the top-left corner, which is ideal.This model can almost perfectly rank fraud vs. non-fraud.Near-perfect AUC means high detection with very low false alarms.</p>


  </p>
    </li>
    <h3>ğŸ“Š Feature Importance</h3>
      <div>
        <h4>1ï¸âƒ£ Gini-based Importance</h4>
        <img src="feature_importance_rf.png" width="700" alt="RF Gini Importance">
        
        <h4>2ï¸âƒ£ Permutation-based Importance</h4>
        <img src="permutation_importance_rf.png" width="700" alt="RF Permutation Importance">
        
    <p><strong>Note:</strong> Both the Giniâ€based and permutationâ€based importance rankings confirm that transaction amount and category are by far the most influential predictors in Random Forest Model, with the other three features contributing only marginally. The Gini scores  somewhat overstate the gap between amt and category_Code, whereas permutation importance reveals theyâ€™re nearly equally critical to model performance. In practice, this means the fraud detector model I built relies almost entirely on those two variables. </p>
    </li>
  </ol>

</p>
  <h3>ğŸ“Š SHAP Analysis ( Fraud Data) </h3>
  <div>
    <img src="SHAP_waterfall1_rf.png" width="700" alt="SHAP Waterfall 1">
  </div>
    <img src="SHAP_waterfall2_rf.png" width="700" alt="SHAP Waterfall 2">
  </div>
    <p><strong> <class="insight">ğŸ” Interpretation of Waterfall Plot:</strong> In each case, the transaction amount (amt) is the single biggest push (+0.52/ +0.53), followed by the category_Code=2 (+0.45/ +0.46). The age_scaled and job_freq features add only a sliver (+0.01 or less), while gender_le slightly suppresses the score. In short, Random Forest flags these two transactions as fraud almost entirely on the basis of an unusually large amount and a highâ€risk categoryâ€”other features barely move the needle.</p>
  
  <h2>ğŸ“¦ Cost Evaluation</h2>
To evaluate the cost performance of the model, I first defined a cost ratio between false negatives (FN) and false positives (FP) as 50:1. In this case, each FN was assigned a cost of $500, and each FP a cost of $10. These values reflect the real-world impact of fraud detection, where failing to detect a fraud (FN) is far more costly than mistakenly flagging a normal transaction (FP).
  <img src="Cost_Sensitivity_lg.png" width="700" alt="Logistic Regression model cost graph">
  
  <p><strong>Note:</strong>  Using these cost values, I calculated the total cost for different decision thresholds on the test set. The graph above shows how total cost changes as the probability threshold varies in the logistic model. It is clear that when the threshold is between 0.0 and 0.2, the total cost stays extremely high, around $3.8 million. This is likely because, at low thresholds, the model predicts too many positives, increasing the number of false positives.As the threshold increases, the total cost drops sharply, reaching the minimum point at a threshold of 0.65. This threshold represents the best trade-off between FPs and FNs, resulting in the lowest total cost. After this point, the total cost begins to rise again, but at a slower rate, remaining below $1.5 million. This suggests that the model performs best when the threshold is set around 0.65, with the minimum cost of $0.38 million helping reduce overall losses from fraud detection errors.<p>
  </div>
  <img src="Cost_Sensitivity_rf.png" width="700" alt="Logistic Regression model cost graph">  
  <p><strong>Note:</strong>  Unlike the logistic regression model, the cost curve for the random forest model appears much smoother, with less fluctuation. As the probability threshold increases, the total cost slightly decreases at first, reaching its minimum at a very low threshold of 0.08. At this point, the minimum total cost is approximately $130,000.
After this optimal threshold, the total cost starts to rise steadily. The cost reaches its highest point when the threshold is close to 1.0. This increase happens because, as the threshold becomes higher, fewer transactions are flagged as fraudulent. As a result, more fraud cases are missed, leading to a growing number of false negatives, which are far more costly in this scenario.
Overall, the graph suggests that the random forest model performs best at a low threshold, around 0.08, where it can catch more fraud cases early and minimize total cost. This highlights the modelâ€™s strength in detecting fraud even with relatively low prediction probabilities.</p>
    





  
  <!-- ğŸ‘¤ ä½œè€… -->
  <div class="footer">
    Created by Andy Wang Â· Published with GitHub Pages
  </div>

</body>
</html>


