<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Credit Card Transactions Fraud Detection Report</title>
</head>
<body>
  <h1>📊 Credit Card Transaction Fraud Detection Data Analysis</h1>
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Fraud Detection EDA Dashboard</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      line-height: 1.6;
      color: #333;
      background-color: #f9f9f9;
    }
    h1 {
      text-align: center;
      color: #1a73e8;
    }
    h2 {
      margin-top: 40px;
      color: #2c3e50;
      border-bottom: 2px solid #ccc;
      padding-bottom: 5px;
    }
    h3 {
      margin-top: 25px;
      color: #444;
    }
    a {
      color: #1a73e8;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      display: block;
      margin: 10px auto 30px;
      border: 1px solid #ccc;
      box-shadow: 2px 2px 8px rgba(0,0,0,0.1);
    }
    p {
      max-width: 800px;
      margin: 0 auto 15px;
      text-align: justify;
    }
    hr {
      margin: 40px 0;
      border: none;
      border-top: 1px solid #ddd;
    }
    .footer {
      text-align: center;
      margin-top: 60px;
      font-style: italic;
      color: #777;
    }
  </style>
</head>
<body>

<h2>📦 Data Resource</h2>
<p class="left-aligned">
  Original dataset from
  <a href="https://www.kaggle.com/kartik2112/fraud-detection" target="_blank">
    Kaggle: Credit Card Fraud Detection
  </a>
</p>

<h2>📄 Interactive EDA Report</h2>
<p class="left-aligned">
  Access the full exploratory data analysis report here:<br>
  <a href="train_eda_report.html" target="_blank">📄 View Full EDA Report for Train Data (HTML)</a>
  <a href="test_eda_report.html" target="_blank">📄 View Full EDA Report for Test Data (HTML)</a>
</p>

 <h2>📊 Class Imbalance Analysis </h2>

<p><p>
<p>The training dataset contains a total of <strong>1,844,974</strong> credit card transaction records labeled as <code>is_fraud</code>, indicating whether a transaction is fraudulent (1) or not (0).</p>

<p>The distribution is as follows:</p>
<ul>
  <li><strong>Non-Fraudulent transactions (label 0):</strong> 1,835,624 (<strong>99.4941%</strong>)</li>
  <li><strong>Fraudulent transactions (label 1):</strong> 9,350 (<strong>0.5059%</strong>)</li>
</ul>

<h3>🧾 Why tackling imbalanced data is important?</h3>
<p>
For a classification model, especially in domains such as fraud detection, medical diagnostics, and spam filtering, rare but high-impact cases are often the primary concern. However, if imbalanced data is not properly addressed during training, the model may appear to perform well—achieving high overall accuracy—while completely failing to detect the minority class. This results in misleading evaluation metrics and significantly reduces the model’s practical value.

In such datasets, the minority class often behaves like an outlier due to its sparse distribution in the feature space. Many models interpret these rare patterns as statistical noise or unrepresentative signals, leading them to ignore or underfit the minority class. Since most algorithms are optimized to minimize overall error, they are inherently biased toward the majority class.

Consequently, imbalanced data can cause the model to overfit the dominant class while neglecting the minority class, which, ironically, is typically the class of greatest importance. Addressing this imbalance is therefore critical to building models that are not only statistically sound but also effective in real-world decision-making.
</p>
 

  <!-- 📈 数据可视化部分 -->
  <h2>📈 Data Visualization Display</h2>

  <h3>1️⃣ Fraud Volume per Transaction Category</h3>
  <img src="fraud_by_category (1).png" width="700">
  <p class="insight">🔍 Fraudulent activity is most concentrated in “grocery_pos” and “gas_transport” categories.</p>

  <h3>2️⃣ 	Top 10 Cities by Number of Fraudulent Transaction </h3>
  <img src="fraud_by_top10_cities (2).png" width="700" alt="Fraud Rate in Top Cities">
  <p class="insight">🔍The fraudulent transactions were evenly distributed across each city, and there was no particularly high concentration in any specific city.</p>
  
  <h3>3️⃣ 	Distribution of Fraudulent Transaction by Log Transform</h3>
  <img src="distribution of fraud transaction by log transform (2).png" width="700" alt="Distribution of Fraudulent Transaction by Log Transform">
  <p class="insight">🔍 Two fraud “hot-spots” emerge.Most cases cluster at log10 ≈ 2.5-3.1 (≈ $320–1 250), suggesting fraudsters prefer high-profit amounts.A smaller spike at log10 ≈ 1.0-1.3 (≈ $8–20) indicates card-testing micro-charges.Few frauds occur between, so risk versus amount is clearly non-linear—models should bin or split on value.
  </p>
  <h3>4️⃣ Top 10 Jobs by Number of Fraudulent Transaction</h3>
<img src="fraud_by_top10_job.png" width="700" alt="Top 10 Jobs by Fraud Number">
  <p class="insight">🔍 The fraudulent transactions were evenly distributed across each job, and there was no particularly high concentration in any specific job.</p>
  
<h3>5️⃣Fraud vs Non-Fraud by Age Interval</h3>
<img src="Fraud vs Non-Fraud by Age Interval (1).png" width="700" alt="Fraud vs Non-Fraud by Age Interval">
  <p class="insight">🔍Age‐interval comparison indicates fraud frequency increases with age, peaking in the 58–95 age group.

<!-- Data Preprocessing -->
<section id="data-cleaning">
  <h2>📦 Data Preprocessing</h2>
  <ol>
    <li>Removed redundant variables and retained only <code>category</code>, <code>amt</code>, <code>gender</code>, <code>job</code>, <code>is_fraud</code>, and <code>age_at_transaction</code> for testing.</li>
    <li>
      <strong>Categorical Variables encoding</strong>
      <ul>
        <li>Considering that <code>category</code> has a relatively small number of subdivisions, I encoded it by mapping each sub-category to a specific integer.</li>
        <li>For <code>job</code>, I initially considered using <code>LabelEncoder</code>, but its integer encoding implies an ordinal relationship among categories, which is not appropriate for this feature. Therefore, I applied frequency encoding instead.</li>
      </ul>
    </li>
    <li>
      <strong>Numerical Variables encoding</strong>
      <ul>
        <li>Scaled <code>amt</code> and <code>age_at_transaction</code> using <code>StandardScaler</code>.</li>
      </ul>
    </li>
  </ol>
</section>

<hr>

<!-- Model Building & Testing -->
<section id="model-building">
  <h2>🤖 Model Building & Testing</h2>

  <div id="logistic-regression">
    <h3>1️⃣ Logistic Regression</h3>

    <ol>
      <li>Used all remaining variables to train the model, but the classification report performance was unsatisfactory. 
          <img src="log_reg_class_rep1.jpg" width="700" ></li>
      </p>
      <li>Assessed feature importance by examining the magnitudes of the model’s coefficients.</li>
      </p>
      <li>Tried optimizing the model by combining Precision–Recall curve threshold selection with over- and under-sampling. 
         <img src="log_reg_class_rep2.jpg" width="700" ></li>
    </ol>
</p>
    <section id="coef-ci">
  <h3>Confidence Interval of Coefficients</h3>

  <h4>Baseline Fraud Risk (~1.8 %)</h4>
  <p>
    This is the model’s “starting point” probability of fraud when all input factors
    (amount, category, gender, job frequency, age) are at their reference level
    (i.e. zero on the scaled/encoded scale). Converting the intercept (–3.99) into a
    probability gives about <strong>1.8 %</strong>, meaning that without any other information,
    the model would guess just under a 2 % chance of fraud.
  </p>

  <h4>Confidence Intervals (CIs)</h4>
  <p>
    A 95 % confidence interval is a range around each coefficient that shows how precise
    that estimate is. If the range doesn’t cross zero, we’re reasonably sure (95 % confident)
    that the real effect isn’t just random noise. In my results, all intervals stayed
    either entirely above or entirely below zero, so each factor has a real impact on fraud risk.
  </p>

  <h4>Key Feature Effects</h4>
  <ul>
    <li>
      <strong>Transaction Amount (amt):</strong>
      Higher amounts increase fraud risk. Every one-unit jump in your scaled amount
      raises the odds of fraud by about <strong>58 %</strong>.
    </li>
    <li>
      <strong>Merchant Category (category_Code):</strong>
      Some categories are slightly safer. Moving up one code reduces fraud odds by
      about <strong>15 %</strong>.
    </li>
    <li>
      <strong>Gender (gender_le):</strong>
      Depending on how you coded it, going from 0→1 increases fraud odds by roughly
      <strong>23 %</strong>.
    </li>
    <li>
      <strong>Job Frequency (job_freq):</strong>
      This feature nearly guarantees non-fraud in your data—higher values push fraud
      probability toward zero.
    </li>
    <li>
      <strong>Age (age_scaled):</strong>
      Older customers have a slightly higher fraud risk, with each unit of scaled age
      adding about <strong>16 %</strong> to the odds.
    </li>
  </ul>
</section>

    <h3>📊 Key Takeaways</h3>

    </h4>Reading Materials:</strong><h4>
      <li><a href="https://builtin.com/data-science/roc-curves-auc" target="_blank">ROC Curves & AUC</a></li>
      <li><a href="https://builtin.com/data-science/precision-and-recall" target="_blank">Precision and Recall</a></li>
    </ul>

    <h4>What Is an ROC Curve and AUC?</h4>
    <p>An ROC curve (receiver operating characteristic curve) measures a classifier’s performance by plotting the true positive rate against the false positive rate at various thresholds. AUC (area under the ROC curve) represents the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.</p>

    <h4>Why Choose the ROC Curve?</h4>
    <p>AUC is <strong>threshold-invariant</strong> (the metric’s value does not depend on a particular decision threshold) and <strong>scale-invariant</strong> (the metric evaluates ranking quality rather than absolute score values).</p>

    <h4>Trade-off Between TPR and FPR</h4>
    <p>If your goal is to identify as many true positives as possible, you pursue a high TPR, which may come at the cost of a higher FPR—meaning false positives become more frequent. And vice versa.</p>

    <h4>Personal Insights</h4>
    <ul>
      <li><strong>TPR</strong> indicates the proportion of actual positives that the model correctly predicts as positive. <strong>FPR</strong> indicates the proportion of actual negatives that the model incorrectly predicts as positive.</li>
      <li><strong>Precision</strong>: Of all samples predicted positive (TP + FP), what fraction are truly positive (TP).</li>
      <li><strong>Recall</strong>: Of all actual positive samples (TP + FN), what fraction did the model correctly predict as positive (TP).</li>
    </ul>

    
    <h3>🔑 Evaluating the Logistic Regression Model: Confusion Matrix, ROC & AUC, and PR Curve</h3>

    <h4>Confusion Matrix</h4>
<img src="confusion_metric.jpg" width="700" ></li>

    <h4>ROC Curve</h4>
  <img src="ROC Curve.png" width="700" ></li>
<div style="margin-left: 20px;">
  <p><strong>Key takeaway:The curve is moderately close to the top-left corner.This suggests the model is pretty good at distinguishing fraud, but there’s room for improvement.True Positive Rate (TPR) starts high but flattens early, indicating some trade-off with increasing False Positive Rate (FPR).</p>


    <h4>PR Curve</h4>
 <img src="PR Curve.png" width="700" ></li>
    <p><strong>Key takeaway:</strong> An Average Precision of 0.19 means that—even though you can rank frauds relatively well—the model’s practical precision is low: when it flags a transaction as fraud, it’s correct only 19% of the time on average.</p>


    </p>
    <h3>📝 Comment</h3>
    <p>Although the model achieves strong overall discriminative power (ROC-AUC = 0.84), precision on the minority class remains low (AP = 0.19), so logistic regression is not an effective tool to predict credit card fraud.</p>
  </div>
</section>
  </p>
  <!-- Random Forest -->
<div id="random-forest">
  <h3>2️⃣ Random Forest</h3>
    <ol>
      <li>Using the Random Forest with only class_weight='balanced', the model achieved a precision of 0.84 and a recall of 0.69. This configuration prioritized reducing false negatives, only correctly flagging 69% of fraudulent transactions while maintaining a high level of precision.</li>
          <img src="rf_class_rep1.jpg" width="700" ></li>
      <li>Next, I applied both SMOTE (oversampling the minority class at a 0.5 ratio) and random undersampling of the majority class. While this boosted recall to 0.87—meaning the vast majority of fraud cases were detected—it came at the cost of precision dropping to 0.41. In practical terms, many non-fraudulent transactions were now being misclassified as fraud.</li>
                <img src="rf_class_rep2.jpg" width="700" ></li>
      <li>Finally, I tried to remove undersampling and reduce the SMOTE ratio from 0.5 to 0.1. I then used the Precision–Recall curve to identify the optimal decision threshold for maximizing the F1 score. With this tuned setup, the model struck a better balance, yielding a precision of 0.766 and a recall of 0.723.
         <img src="rf_class_rep3.jpg" width="700" ></li>
    </ol>
  </p>
    <h3>📊 Evaluating the Random forest Model: ROC & AUC and PR Curve,</h3>
  
      <h4>PR Curve</h4>
  <img src="PR_Cuver_rf.png" width="700" alt="RF Confusion Matrix">
   <p><strong>Key takeaway:</strong> At very low recall , we can get near-perfect precision, but beyond ~0.7 recall precision dips below 0.8. The red dot marks the threshold (≈0.73) that maximizes F₁, giving us about precision ≈ 0.77 and recall ≈ 0.73. Thus, we can strike a balance—catch roughly 73% of frauds while still being right about 77% of the time.</p>
           
     <h4>ROC Curve</h4>
  <img src="ROC_Curve_rf.png" width="700" alt="RF ROC & AUC">
    <p><strong>Key takeaway:</strong> The curve almost hugs the top-left corner, which is ideal.This model can almost perfectly rank fraud vs. non-fraud.Near-perfect AUC means high detection with very low false alarms.</p>


  </p>
    </li>
    <h3>📊 Feature Importance</h3>
      <div>
        <h4>1️⃣ Gini-based Importance</h4>
        <img src="feature_importance_rf.png" width="700" alt="RF Gini Importance">
        
        <h4>2️⃣ Permutation-based Importance</h4>
        <img src="permutation_importance_rf.png" width="700" alt="RF Permutation Importance">
        
    <p><strong>Note:</strong> Both the Gini‐based and permutation‐based importance rankings confirm that transaction amount and category are by far the most influential predictors in Random Forest Model, with the other three features contributing only marginally. The Gini scores  somewhat overstate the gap between amt and category_Code, whereas permutation importance reveals they’re nearly equally critical to model performance. In practice, this means the fraud detector model I built relies almost entirely on those two variables. </p>
    </li>
  </ol>

</p>
  <h3>📊 SHAP Analysis ( Fraud Data) </h3>
  <div>
    <img src="SHAP_waterfall1_rf.png" width="700" alt="SHAP Waterfall 1">
  </div>
    <img src="SHAP_waterfall2_rf.png" width="700" alt="SHAP Waterfall 2">
  </div>
    <p><strong> <class="insight">🔍 Interpretation of Waterfall Plot:</strong> In each case, the transaction amount (amt) is the single biggest push (+0.52/ +0.53), followed by the category_Code=2 (+0.45/ +0.46). The age_scaled and job_freq features add only a sliver (+0.01 or less), while gender_le slightly suppresses the score. In short, Random Forest flags these two transactions as fraud almost entirely on the basis of an unusually large amount and a high‐risk category—other features barely move the needle.</p>
  
  <h2>📦 Cost Evaluation</h2>
To evaluate the cost performance of the model, I first defined a cost ratio between false negatives (FN) and false positives (FP) as 50:1. In this case, each FN was assigned a cost of $500, and each FP a cost of $10. These values reflect the real-world impact of fraud detection, where failing to detect a fraud (FN) is far more costly than mistakenly flagging a normal transaction (FP).
  <img src="Cost_Sensitivity_lg.png" width="700" alt="Logistic Regression model cost graph">
  
  <p><strong>Note:</strong>  Using these cost values, I calculated the total cost for different decision thresholds on the test set. The graph above shows how total cost changes as the probability threshold varies in the logistic model. It is clear that when the threshold is between 0.0 and 0.2, the total cost stays extremely high, around $3.8 million. This is likely because, at low thresholds, the model predicts too many positives, increasing the number of false positives.As the threshold increases, the total cost drops sharply, reaching the minimum point at a threshold of 0.65. This threshold represents the best trade-off between FPs and FNs, resulting in the lowest total cost. After this point, the total cost begins to rise again, but at a slower rate, remaining below $1.5 million. This suggests that the model performs best when the threshold is set around 0.65, with the minimum cost of $0.38 million helping reduce overall losses from fraud detection errors.<p>
  </div>
  <img src="Cost_Sensitivity_rf.png" width="700" alt="Logistic Regression model cost graph">  
  <p><strong>Note:</strong>  Unlike the logistic regression model, the cost curve for the random forest model appears much smoother, with less fluctuation. As the probability threshold increases, the total cost slightly decreases at first, reaching its minimum at a very low threshold of 0.08. At this point, the minimum total cost is approximately $130,000.
After this optimal threshold, the total cost starts to rise steadily. The cost reaches its highest point when the threshold is close to 1.0. This increase happens because, as the threshold becomes higher, fewer transactions are flagged as fraudulent. As a result, more fraud cases are missed, leading to a growing number of false negatives, which are far more costly in this scenario.
Overall, the graph suggests that the random forest model performs best at a low threshold, around 0.08, where it can catch more fraud cases early and minimize total cost. This highlights the model’s strength in detecting fraud even with relatively low prediction probabilities.</p>
    





  
  <!-- 👤 作者 -->
  <div class="footer">
    Created by Andy Wang · Published with GitHub Pages
  </div>

</body>
</html>


